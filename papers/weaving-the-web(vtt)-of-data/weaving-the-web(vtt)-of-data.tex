\documentclass{sig-alternate}

\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=2em,belowskip=1em,abovecaptionskip=0.5em,belowcaptionskip=0.5em,
        framexbottommargin=-1em,basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

\lstdefinelanguage{JavaScript}{
  keywords={push, typeof, new, true, false, catch, function, return, null,
    catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{darkgray},
  stringstyle=\color{red},
  morestring=[b]',
  morestring=[b]"
}

% linewrap symbol
\usepackage{color}
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}

% black diamond with white question mark
\usepackage{graphicx,color}
\makeatletter
\newsavebox\Diam@nd \newlength\x@D\newlength\y@D
\newcommand\unknown{%
  \sbox\Diam@nd{\raisebox{-1ex}{\scalebox{1}[1.2]{\rotatebox{45}{\rule{0.8em}{0.8em}}}}}%
  \makebox[\wd\Diam@nd]{\makebox[0pt]{\usebox\Diam@nd}\makebox[0pt]{\textcolor{white}{?}}}}
\makeatother

\hyphenation{WebVTT}

\newcommand{\vtt}[1]{\texttt{vtt:#1}}
\def\JSONLD{\mbox{JSON-LD}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{International World Wide Web Conference}{2014 Seoul, Korea}
\CopyrightYear{2014} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Weaving the Web(VTT) of Data}

\numberofauthors{6}

\author{
% 1st. author
\alignauthor
Thomas Steiner\titlenote{Second affiliation: \emph{Google Germany GmbH, Hamburg, DE}}\\
       \affaddr{CNRS, Université de Lyon}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{Université Lyon~1, France}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont tsteiner@liris.cnrs.fr}
% 2nd. author
\alignauthor
Hannes Mühleisen\\
       \affaddr{Database Architectures Group}\\
       \affaddr{CWI, Science Park 123}\\
       \affaddr{1098 XG Amsterdam, NL}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont hannes@cwi.nl}
% 3rd. author
\alignauthor
Ruben Verborgh\\
       \affaddr{Multimedia Lab}\\
       \affaddr{Ghent University -- iMinds}\\
       \affaddr{B-9050 Gent, Belgium}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont ruben.verborgh@ugent.be}
\and
% 4th. author
\alignauthor
Pierre-Antoine Champin\\
       \affaddr{CNRS, Université de Lyon}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{Université Lyon~1, France}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont pachampin@liris.cnrs.fr}
% 5th. author
\alignauthor
Benoît Encelle\\
       \affaddr{CNRS, Université de Lyon}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{Université Lyon~1, France}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont bencelle@liris.cnrs.fr}
% 6th. author
\alignauthor
Yannick Prié\\
       \affaddr{LINA -- UMR 6241 CNRS}\\
       \affaddr{Université de Nantes}\\
       \affaddr{44322 Nantes Cedex 3}\\
       \email{\fontsize{12pt}{14.4pt}\sffamily\selectfont yannick.prie@univ-nantes.fr}
}

\maketitle
\begin{abstract}
We propose that WebVTT data can be considered as a rich, and yet still untapped, source of video-related linked data. Indeed, Web standards such as Media Fragments and the Media Annotation ontology make it quite straightforward to rephrase WebVTT as linked data. We present in this paper the principles of this transformation. Then we provide evidence, based on the Common Crawl corpus, of the huge potential of WebVTT as a source of linked data. Finally, we propose a prototype, based on modern Web technology (HTML5 and JSON-LD), easing the consumption \emph{and} production of WebVTT-encoded linked data.

\end{abstract}

\category{H.5.1}{Multimedia Information Systems}{Video}

%\terms{Experimentation, Design, Standardization}

\keywords{WebVTT, Linked Data, Web of Data}

\section{Introduction}

\cite{pfeiffer2013webvtt}

\section{Technologies Overview}

For our approach to online video annotation,
we make use of a~young---yet mature
when looked at from an implementation viewpoint---%
stack of enabling technologies that we will introduce
and provide technical details on in the following.

\paragraph{Web Video Text Tracks format (WebVTT)}

The Web Video Text Tracks format (WebVTT,~\cite{pfeiffer2013webvtt})
is intended for marking up external text track resources mainly
for the purpose of captioning video content.
The recommended file extension is \texttt{vtt},
the MIME type is \texttt{text/vtt}.
WebVTT files are encoded in UTF-8 and
start with the required string \texttt{WEBVTT}.
Each file consists of items called \emph{cues}
that are separated by an empty line.
Each cue has a~start time and an end time in
\texttt{hh:mm:ss.milliseconds} format,
separated by a~stylized ASCII arrow \texttt{-}\texttt{->}.
The cue payload follows in the line after the cue timings part
and can span multiple lines.
Typically, the cue payload contains plain text,
but can also contain textual data serialization formats like JSON,
which later on in the paper we will show is essential
for our proposed approach to semantic video annotation.
%Spans of text associated with a~specific voice can be annotated
%in form of WebVTT cue voice spans.
%A~WebVTT voice tag, denoted by \texttt{<v VOICE>}, has a~value,
%which is the name of the voice.
Cues optionally can have unique WebVTT identifiers.

\begin{comment}
\begin{table}[b!]\footnotesize
\begin{tabular}{ r p{5.5cm} } % ACM column width 8.45cm, 0.83cm gutter
\textbf{WebVTT Kind} & \textbf{Description and Default Behavior}\\

\texttt{subtitles} & Transcription or translation of speech,
suitable for when sound is available but not understood.
Overlaid on the video.\\

\texttt{captions} & Transcription or translation of the dialogue,
sound effects, and other relevant audio information,
suitable for when sound is unavailable or not clearly audible.
Overlaid on the video;
labeled as appropriate for the hard-of-hearing.\\

\texttt{descriptions} & Textual descriptions of the video component
of the media resource, intended for audio synthesis
when the visual component is obscured, unavailable, or unusable.
Synthesized as audio.\\

\texttt{chapters} & Chapter titles, intended to be used for navigating
the media resource. Displayed as an interactive (potentially nested)
list in the user agent's interface.\\

\texttt{metadata} & Metadata intended for use from script context.
Not displayed by user agent.\\
\end{tabular}
  \caption{WebVTT text track kinds in HTML5~\cite{berjon2013html5}}
  \label{table:texttrackkinds}
\end{table}
\end{comment}

WebVTT-compliant Web browsers %~\cite{dutton2012trackelement}
support five different kinds of
WebVTT tracks: \emph{subtitles}, \emph{captions},
\emph{descriptions}, \emph{chapters}, and \emph{metadata},
all specified in HTML5~\cite{berjon2013html5}.
In this paper, we especially focus on
text tracks of kind \texttt{metadata}
that are meant to be used from a~scripting context and
that are not displayed by the user agent.

Both the HTML5 \texttt{<audio>} and \texttt{<video>} elements
have a~\texttt{textTracks} property
that returns a~\texttt{TextTrackList} of
\texttt{TextTrack} members, each of which correspond
to individual <track> elements.
A~\texttt{TextTrack} has a~\texttt{cues} property
that returns a~\texttt{TextTrackCueList} of individual
\texttt{TextTrackCue} items.
Cue data is accessible via properties like
\texttt{startTime}, \texttt{endTime} and,
most importantly, \texttt{text} to obtain a~cue's payload.
When the payload is in JSON format,
it can be parsed via the
\texttt{JSON.parse()} function.

When a~media resource like a~video or audio is playing,
JavaScript events of the \texttt{TextTrack} and \texttt{TextTrackCue}
elements fire automatically when the \texttt{currentTime}
of the media resource matches
a~cue's \texttt{startTime} or \texttt{endTime}.
\texttt{TextTrack} elements fire \texttt{oncuechange} events,
whereas \texttt{TextTrackCue} elements fire
\texttt{onenter} and \texttt{onexit} events.
JavaScript applications can subscribe to these events.
Important for us, both \texttt{TextTrack} and
\texttt{TextTrackCue} elements
can be dynamically generated via JavaScript.

\paragraph{\JSONLD}

The \emph{JavaScript Object Notation}%
\footnote{JavaScript Object Notation: \url{http://json.org/}}
(JSON)
is a~(despite the name) language-independent textual syntax
for serializing objects, arrays, numbers, strings, booleans, and null.
\emph{Linked Data}~\cite{bizer2009linkeddata}
describes a~method of publishing structured data
so that it can be interlinked and become more useful,
which builds upon standard Web technologies such as HTTP, RDF and URIs.
Based on top of JSON, the
\emph{JavaScript Object Notation for Linked Data}
(\JSONLD,~\cite{sporny2013jsonld}) is a~method for transporting
Linked Data with a~smooth upgrade path from JSON to \JSONLD.
\JSONLD~properties like \texttt{title} can be mapped to taxonomic
concepts (like \texttt{dc:title}) via so-called data contexts.
We use \JSONLD\ for semantically rich content
in the payload of \texttt{TextTrackCue}s, shown in
\autoref{listing:webvtt}.
The referenced local context defines the semantics.

\paragraph{Media Fragments URI}

Media Fragments URI~\cite{troncy2012mediafragments}
specifies a~syntax for constructing media fragments URIs
and explains how to handle them over the HTTP protocol.
The syntax is based on the specification of
name-value pairs that can be used in URI query strings
and URI fragment identifiers to restrict a~media resource
to a~certain fragment.
Media Fragments URI supports temporal and spatial media fragments.
The \emph{temporal dimension} is denoted
by the parameter name \texttt{t} and specified
as a~half-open interval with begin time and end time
that may also be omitted,
with the begin time defaulting to 0 seconds
and the end time defaulting to the media item's duration.
The \emph{spatial dimension} selects
a~rectangular area of pixels from media items.
Rectangles can be specified as pixel coordinates or percentages.
Rectangle selection is denoted by the parameter name \texttt{xywh}.
The value is either \texttt{pixel:} or \texttt{percent:}
(defaulting to \texttt{pixel:})
followed by four comma-separated integers.
The integers denote $x$, $y$, $width$, and $height$ respectively,
with $x = 0$ and $y = 0$ being the top left corner of the media item.
If \texttt{percent:} is used, $x$ and $width$ are interpreted
as a~percentage of the width of the original media item,
$y$ and $height$
of the original height.

\paragraph{Ontology for Media Resources}

The Ontology for Media Resources~\cite{lee2012mediaontology}
serves to bridge different description methods of media resources
and to provide a~core set of descriptive properties.
It also defines mappings to common metadata formats.
Combined with Media Fragments URI,
this allows for making ontologically anchored statements
about media items and fragments thereof.

\begin{lstlisting}[caption={Sample WebVTT metadata file},
  label=listing:webvtt, float=t!]
WEBVTT

cue1
00:00:00.000 --> 00:00:12.000
{
  "@context": "http://ex.org/local_context",
  "tags": ["wind scene", "opening credits"],
  "contributors": ["http://ex.org/sintel"]
}
\end{lstlisting}

\begin{lstlisting}[caption={Generated \JSONLD~for the Web of Data, see
    \url{http://bit.ly/webvtt-jsonld} for the RDF triples},
  label=listing:jsonld, float=t!]
{
  "@context": "http://ex.org/global_context",
  "@id": "http://ex.org/video",
  "@type": "MediaResource",
  "hasFragment": [{
    "@id": "http://ex.org/video#t=0,12",
    "@type": "MediaFragment",
    "@context": "http://ex.org/local_context",
    "tags": ["wind scene", "opening credits"],
    "contributors": ["http://ex.org/sintel"]
  }]
}
\end{lstlisting}

\section{Annotation Model}

\paragraph{General Concepts of the Annotation Model}

Our annotation model uses subject-predicate-object expressions.
An annotation is thus a~single triple.
The possible values for the \emph{subject} can be \emph{(i)}~the video
URI itself or---in case of multiple URIs for a~video available
in different encodings with multiple \texttt{<source>} tags---%
the URI with fragment identifier of the \texttt{<video>} tag
that then requires a~unique \texttt{id} attribute,
\emph{(ii)}~the Media Fragment URI of a~temporal and/or spatial
media fragment of the original media item.
We have no constraints in place for the \emph{predicate},
which determines the annotation type, and the \emph{object}.
In our interpretation layer responsible for the video view,
we thus make an open world assumption
by supporting a~set of pre-defined values for predicate and object
listed below, and ignoring unknown ones.
This permits others to extend---or even completely replace---%
our interpretation layer.
If a~\texttt{TextTrackCue} has a~WebVTT identifier,
we use it to address its annotations
via the metadata track's URI
and corresponding cue fragment identifier,
allowing for meta annotations of annotations, \emph{e.g.},
to attach provenance or license information to them.

\paragraph{Supported Semantic Annotation Types}

Our model makes common annotation tasks as
straight-forward as in \autoref{listing:webvtt}.
The thereby generated triples are shown in
\autoref{listing:jsonld} and \url{http://bit.ly/webvtt-jsonld}.
Advanced tasks are supported by extending \JSONLD~contexts.\\

\noindent \textbf{Plain Text Tags:} Annotations of type
  \texttt{tags} allow for add\-ing plain text tags
  to a~media fragment.
  Internally, they are represented as Common Tag%
  ~\cite{commontag2009spec} format \texttt{ctag:label}.

\noindent \textbf{Semantic Tags:} Annotations of type
  \texttt{semanticTags} allow for adding semantic tags
  to a~media fragment.
  Unlike plain text tags, semantic tags are references to
  well-defined concepts complete with their own URIs.
  Internally, they are represented as Common Tag%
  ~\cite{commontag2009spec} format \texttt{ctag:means}.

\noindent \textbf{Contributors:} The \texttt{contributors} annotation type
  al-\linebreak lows for denoting the contributors in a~media fragment, like its actors.
  Internally, they are represented as
  Ontology for Media Resources~\cite{lee2012mediaontology}
  format \texttt{ma:contributors}.

\noindent \textbf{Summary:} The \texttt{summary} annotation type
  allows for summarizing a~media fragment
  (note, not the whole video like kind \emph{description} tracks)
  with plain text.
  Internally, they are represented as  
  \texttt{ma:description}~\cite{lee2012mediaontology}.

\noindent \textbf{Named Graph:} The \texttt{namedGraph} annotation type
  allows for adding a~set of RDF triples
  to a~media fragment.
  Internally, they are represented as \JSONLD%
  ~\cite{sporny2013jsonld} named graph.

\paragraph{Supported Presentation-Oriented Annotation Types}

Presentation-oriented annotations---%
similar to temporal\linebreak style sheets
(also see \autoref{sec:future-work-conclusions})---%
do not generate RDF data,
but only impact the way videos get presented.\\

\noindent \textbf{Visual Effect:} Annotations of
  type \texttt{visualEffect} allow for applying visual effects
  in the syntax of Cascading Style Sheets%
  \footnote{Cascading Style Sheets:
  \url{http://www.w3.org/Style/CSS/}} (CSS)
  to a~media fragment, \emph{e.g.},
  filters, zoom effects, transparency,
  and 2D/3D transformations and animations.

\noindent \textbf{Audial Effect:} The \texttt{audialEffect} annotation type
  allows for applying audial effects to a~media fragment.
  Currently, we support modifying the volume on a~scale
  from~0 to~1.

\noindent \textbf{Playback Rate:} The \texttt{playbackRate}
  annotation type allows for specifying the effective
  playback rate of a~media fragment.
  The playback rate is expressed as a~floating point
  multiple or fraction of the intrinsic video speed.

\noindent \textbf{HTML Overlay:} Via the \texttt{htmlOverlay}
  annotation type, overlays in
  freeform HTML code can be added to a~media fragment.
  Examples are graphical, textual, or combined overlays
  that can contain links to (temporal fragments of)
  other videos or within the current video.

\section{WebVTT as linked data}

The WebVTT specification defines a syntax for conveying video text tracks,
and a semantics for this syntax in terms of how web browser should process such tracks.
It achieves that by specifying an underlying data model for those tracks.
The aim of this section is to show
how this data model can easily be mapped to RDF-based linked data,
thus allowing many other uses of this data.
For this purpose, we propose an RDF-Schema ontology\footnote{
  \url{http://champin.net/2014/linkedvtt/onto\#}
} conveying the WebVTT data model.
In the rest of the paper, terms of this ontology will be preceded by the \vtt{} prefix.
An online implementation of this interpretation process is also available\footnote{
  \url{http://champin.net/2014/linkedvtt/}
}.

\subsection{Basic interpretation}

A WebVTT file defines a set of cues,
which are described by a pair of timestamps and a payload.
In other word, each cue is an annotation of the video,
associating a fragment, delimited by the two timestamps, to the payload.
As we have a standard way of identifying that fragment with a URI~\cite{troncy2012mediafragments}
it is straightfoward to represent this annotation as an RDF triple.
We therefore propose a property \vtt{annotatedBy} to serve as predicate for those triples.

To keep the context of each annotation,
we use the notion of RDF dataset~\cite{cyganiak2014rdf11concepts}.
Each \vtt{annotatedBy} triple is enclosed in a named graph,
whose name is either a URI, based on the cue identifier it has one,
or a blank node if the cue has no identifier.
The default graph of the dataset describes its overall structure,
linking the dataset URI to all the URIs and blank nodes identifying its cues
with the \vtt{hasCue} property.
Each cue is also linked, in the default graph, to the media fragment URI it describes,
with the \vtt{describesFragment} property. 

\todo{illustrate with a figure, representing the WebVTT example that will probably be there in the intro?}

As the notion of dataset is a recent addition to the RDF core concepts
(previously, it was specific to the SPARQL query language),
we envision that some consumers will not be able to deal with it.
Hence we propose an alternate interpretation of WebVTT into RDF.
In this so-called \emph{flat} interpretation,
the content of all named graph is merged into the default graph.

\subsection{Advanced interpretation}

WebVTT is not limited to textual caption tracks.
The HTML5 \texttt{track} tag supports indeed different kinds of track,
one of them being \textit{metadata},
a track designed for machine rather than human consumption.
Although there is few evidence of use of this kind of tracks (see next section),
which is understandable as the technology is still under development,
we propose that JSON data is a good candidate for cues of such tracks.
Indeed, JSON has a textual syntax which is easy to author
and easy to process in a web browser.
Furthermore, JSON-LD~\cite{sporny2013jsonld} provides
a standard way to interpret JSON data as linked data,
which fits nicely with our approach.

More precisely, whenever the payload of a cue parses to a JSON object,
we consider that this object is meant to represent the annotated media fragment itself,
and we interpret it as JSON-LD.
Hence, all the property of the JSON object are applied directly to the fragment,
and embedded structures can be used to describe other resources related to that fragment
(\textit{e.g.} depicted persons).
In that case, all the triples generated from parsing the payload as JSON-LD
\emph{replace} the \vtt{annotatedBy} triple in the cue's named graph.
Figure~\ref{fig:json-payload} gives an example of such a JSON payload.

\begin{figure}
\begin{verbatim}
00:00:00.000 --> 00:00:12.000
{
 "label": "Vince and Jules in the car",
 "depicts": [
  { "@id":
     "http://www.wikidata.org/entity/Q80938",
    "name": "John Travolta" },
  { "@id":
     "http://www.wikidata.org/entity/Q172678",
    "name": "Samuel L. Jackson" }
  ]
}
\end{verbatim}
\label{fig:json-payload}
\caption{A WebVTT cue with a JSON payload}
\end{figure}

Note that JSON-LD requires a \emph{context} to interpret JSON data as linked data,
especially to disambiguate keywords to full URIs.
This context can of course be provided in each cue,
but we provide a convenient way to declare it once for the entire WebVTT file (see below).

\subsection{Linked-data related metadata}

WebVTT files can contain, in addition to the cues, some metadata described as key-value pairs.
While the WebVTT specification defines a number of metadata keys,
it leaves it open for extensions.
We propose three extended metadata keys:

\begin{description}

\item[@base]
Sets the base URI used for resolving relative URIs.
This applies to any relative URIs that would be found in the JSON-LD descriptions,
but also to generate URIs for cues based on their identifiers.
It defaults to the URI of the WebVTT file.

\item[@context]
This key can be used multiple times; each value is the URI of a JSON-LD context
that should be used to interpret the JSON payloads in the WebVTT file.

\item[@video]
Sets the URI for the video for generating media fragment URIs.
If not present, the video URI must be provided externally,
\textit{e.g.} the \texttt{src} attribute of the \texttt{video} tag
containing the WebVTT track.

\end{description}

Of course, WebVTT in the wild does not contain those metadata keys,
but we believe that they allow an easy transition
from plain WebVTT to linked-data WebVTT,
just like JSON-LD makes it easy to turn plain JSON into linked data
by simply adding a \texttt{@context} key.

\section{Common Crawl}

The Common Crawl Foundation\footnote{Common Crawl: \url{http://commoncrawl.org/}}
is a~non-profit organization founded in 2008 by Gil Elbaz.
Its objective is to democratize access to Web information
by producing and maintaining an open repository of Web crawl data
that is universally accessible and analyzable~\cite{simonite2013commoncrawl}.
All Common Crawl data is stored on Amazon Simple Storage Service (Amazon~S3)%
\footnote{Amazon S3: \url{http://aws.amazon.com/s3/}} and
accessible to anyone via Amazon Elastic Compute Cloud (Amazon~EC2),%
\footnote{Amazon EC2: \url{http://aws.amazon.com/ec2/}}
allowing the data to be bulk downloaded,
as well as directly accessed for map-reduce processing in EC2.
The at time of writing latest dataset was collected at the end of 2013,
contains approximately 2.3~billion webpages and is 148TB in size~\cite{green2014winter}.
Crawl raw data is stored in the Web ARChive format
(WARC,~\cite{iso285002008warc}), an evolution of the previously used
Archive File Format (ARC,~\cite{burner1996arc}),
which was developed at the Internet Archive.%
\footnote{Internet Archive: \url{https://archive.org/}}
Each crawl run is hierarchically organized in segments directories
that contain the WARC files with the HTTP requests and responses for each fetch,
and individual Web Archive Metadata (WAT,~\cite{goel2011wat}) files,
which describe the metadata of each request and response.
While the Common Crawl corpus gets bigger with each crawl run,
it obviously does not represent the ``whole Web'',
which is an illusive concept anyway,
given that a~simple calendar webpage can produce an infinite number of pages.
Common Crawl decides on the to-be-included pages based on an implementation%
\footnote{Common Crawl PageRank implementation:
\url{https://github.com/commoncrawl/commoncrawl-crawler/tree/master/src/org/commoncrawl/service/pagerank}}
of the PageRank~\cite{page1999pagerank} algorithm.

\subsection{On the Quest for WebVTT}

We have analyzed the entire 148TB of webpages
using an Elastic Compute Cloud job
whose code we have made available as open-source.%
\footnote{EC2 job:
\url{https://github.com/tomayac/postdoc/blob/master/demos/warczenschwein/src/main/java/com/tomayac/warczenschwein/TagTool.java}}
Rather than parse each document as HTML,
we have tested each document for the regular expression
\texttt{<video[\^{}>]*>(.*?)</video>},
which proved very efficient.
We tested exactly 2,247,615,323 webpages
that returned a successful HTTP response,
and had to skip exactly 46,524,336  non-HTML documents.
On these webpages, we detected exactly
2,963,766 \texttt{<video>} tags,
resulting in a~1.37GB size raw text file
that we have made available publicly.%
\footnote{2,963,766 \texttt{<video>} tags: \todo{Put this on DropBox}}
The whole job took about five hours on
80~c1.xlarge machines and costed overall \$555,
consisting of \$468 for Amazon~EC2
plus an additional \$87 for Amazon Elastic MapReduce
 (Amazon EMR)\footnote{Amazon EMR:
\url{http://aws.amazon.com/elasticmapreduce/}}

\subsection{Text Track Statistics}

From all 2,963,766 \texttt{<video>} tags,
only 1,456 had a~\texttt{<track>} child node.
By closer examining these 1,456 \texttt{<track>} nodes,
we saw that the overwhelming majority are unsurprisingly
used for subtitles or captions.
Close to no chapter usage was detected
and no metadata usage at all.
The full details can be seen in \autoref{table:kind}.
The caption and subtitle languages were
almost exclusively English and French,
as can be seen in \autoref{table:srclang},
the track labels listed in \autoref{table:label}
indeed confirm this observation.
In case of multiple tracks for one video,
one track can be marked as the default track.
This happens through a~boolean attribute,%
\footnote{HTML boolean attributes:
\url{http://www.whatwg.org/specs/web-apps/current-work/\#boolean-attributes}}
whose value either needs to be the empty string
or the attribute's name,
which is ``default'' in the concrete case.
\autoref{table:default}
shows that this was used correctly in almost all cases.
When we tried to determine the MIME type of the actual
text tracks, we relied on the file extension
of the values given in \texttt{<track src>}.
As a~significant amount of text tracks
seems to be dynamically generated on-the-fly---%
and thus had no file extension---%
we used an approximation to check if some part
of the URL matched the regular expression \texttt{/\textbackslash bvtt\textbackslash b/gi}.
Based on this approximation,
a~little over half of all text tracks
are in WebVTT format
with the extension \texttt{.vtt}
or rarely \texttt{.webvtt}.
The predecessor SubRip file format%
\footnote{SubRip file format:
\url{http://www.matroska.org/technical/specs/subtitles/srt.html}}
can still be encountered in about
a~quarter of all text tracks.
The full details are available in \autoref{table:src}.
Almost all videos with at least one text track
had only exactly one text track and not more,
as can be seen in \autoref{table:track}.

\subsection{Video Statistics}

As in \todo{reference section once written}
we will report on ways to make semantic statements
about videos on the Web,
we have compiled some video statistics.
Unlike with images on the Web, where
semantic statements in
Resource Description Framework (RDF)
can be made based on the image's
URL~\cite{linsley2009rdfa},
with Web video, the situation is another.
Due to different Web browsers supporting
different video codecs,
it is a~common practice to provide videos
in different encodings.
The user's Web browser than dynamically selects
a~version it can play.
This is realized through the \texttt{<source>} tag.
\autoref{table:source} shows our observed numbers of
\texttt{<source>} tag child nodes per
\texttt{<video>} tag, with the result that up to
four sources are given for essentially the ``same'' video.
\autoref{table:type} shows the distribution
of values for the \texttt{<source type>} attribute.
The biggest issue with this approach is that
rather than having one unique identifier per video,
all of a~sudden there can be multiple.
We note that stating that a~video in encoding~A
and the ``same'' video in encoding~B
may not be marked \texttt{<owl:sameAs>},
because statements about the encoding format
of the one video do not apply to the other,
the symmetry condition would thus be violated.



\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    \texttt{<track kind>} & Count \\
    \hline  
    captions & 915\\
    subtitles & 525\\
    chapters & 2\\
    undefined & 10\\  
  \end{tabular}
  \caption{Distribution of values for
    \texttt{<track kind>}}
  \label{table:kind}
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    \texttt{<track srclang>} & Count \\
    \hline  
    en & 1242\\
    fr & 117\\
    de & 8\\    
    en-US & 3\\
    th & 2\\
    Español & 1\\
    Korean & 1\\
    undefined & 78\\    
  \end{tabular}
  \caption{Distribution of values for
    \texttt{<track srclang>}}
  \label{table:srclang}    
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    \texttt{<track label>} & Count \\
    \hline  
    English & 1064\\
    `Français' & 117\\    
    `English Captions' & 20\\    
    Captions & 10\\
    ```English''' & 3\\    
    `English closed captioning' & 3\\    
    Deutsch & 2\\    
    `English Subtitles' & 2\\
    Thai & 2\\    
    `English ' & 1\\
    `English subtitles' & 1\\
    `\textbackslash\textbackslash\textbackslash`English\textbackslash\textbackslash\textbackslash'' & 1\\
    `Gr\unknown\unknown e' & 1\\
    undefined & 229\\
  \end{tabular}
  \caption{Distribution of values for
    \texttt{<track label>} \tiny (double/triple-quotes,
    backslashes, spacing, and unknown encoding
    \emph{sic})}
  \label{table:label}    
\end{table}
  
\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    \texttt{<track default>} & Count \\
    \hline
    default & 650\\
    `' & 526\\
    true & 1\\
    undefined & 279
  \end{tabular}
  \caption{Distribution of values for
    \texttt{<track default>}}
  \label{table:default}    
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    File extensions of \texttt{<track src>} & Count \\
    \hline
    probably \texttt{.vtt} & 696\\
    \texttt{.srt} & 390\\
    \texttt{.vtt} (or \texttt{.webvtt}) & 66\\
    no extension & 304\\
  \end{tabular}
  \caption{Distribution of file extensions for
    values for \texttt{<track src>}}
  \label{table:src}    
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }                       
    \texttt{<track>} tags & Count \\
    \hline
    1 & 1446\\
    0 & 9\\
    9 & 1\\
  \end{tabular}
  \caption{Number of \texttt{<track>} tags per
    \texttt{<video>} tag \tiny (zero \texttt{<track>}
    tags means  the \texttt{<video>} tag had
    an  unparseable \texttt{<track>})}
  \label{table:track}    
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }
    \texttt{<source>} tags & Count \\
    \hline
    1 & 826\\    
    3 & 404\\    
    2 & 173\\    
    0 & 49\\
    4 & 4\\
  \end{tabular}
  \caption{Number of \texttt{<source>} tags per
    \texttt{<video>} tag \tiny (zero \texttt{<source>}
    tags means the video URL was provided via
    \texttt{<video src>}; 1,405 videos did not have
    a~\texttt{src} attribute, 51 videos had one)}
  \label{table:source}    
\end{table}

\begin{table}[p]
  \centering
  \begin{tabular}{ r | r }
    \texttt{<source type>} & Count \\
    \hline
    video/mp4 & 1279\\
    video/webm & 94\\
    video/x-ms-wmv & 10\\    
    video/ogg & 5\\    
    ``video/mp4'' & 3\\    
    video/mp4; codecs=``avc1.42E01E, mp4a.40.2'' & 3\\
    video/flv & 2\\
    video/youtube & 1\\    
    \textbackslash\textbackslash\textbackslash`video/mp4\textbackslash\textbackslash\textbackslash' & 1\\
    undefined & 58\\  
  \end{tabular}
  \caption{Distribution of values for
    \texttt{<source type>}  \tiny
    (double/triple-quotes,
    backslashes, spacing, and unknown encoding
    \emph{sic})}
  \label{table:type}    
\end{table}

\section*{Acknowledgments}
\footnotesize
The research presented in this paper was partially supported
by the French National Agency for Research  project
\emph{Spectacle En Ligne(s)}, project reference
\mbox{ANR-12-CORP-0015}.

\normalsize
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
