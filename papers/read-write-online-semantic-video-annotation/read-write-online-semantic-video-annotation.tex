\documentclass{sig-alternate}

\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=2em,belowskip=1em,abovecaptionskip=0.5em,belowcaptionskip=0.5em,
        framexbottommargin=-1em,basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

\lstdefinelanguage{JavaScript}{
  keywords={push, typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{darkgray},
  stringstyle=\color{red},
  morestring=[b]',
  morestring=[b]"
}

% linewrap symbol
\usepackage{color}
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

\newcommand{\todo}[1]{\fcolorbox{blue}{white}{\parbox{7cm}{\color{blue}{#1}}}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{International World Wide Web Conference}{2014 Seoul, Korea}
\CopyrightYear{2014} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Read/Write Semantic and Representational Online Video Annotation by Leveraging WebVTT and JSON-LD}

\numberofauthors{6}

\author{
% 1st. author
\alignauthor
Thomas Steiner\titlenote{Thomas Steiner's second affiliation is \emph{T.~Steiner, Google Germany GmbH, ABC-Str.~19, 20354 Hamburg, Germany}}\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{tsteiner@liris.cnrs.fr}
% 2nd. author
\alignauthor
Pierre-Antoine Champin\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{champin@liris.cnrs.fr}
% 3rd. author
\alignauthor
Benoît Encelle\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{encelle@liris.cnrs.fr}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor
Yannick Prié\\
       \affaddr{Université de Nantes}\\
       \affaddr{LINA - UMR 6241}\\
       \affaddr{CNRS, France}\\
       \email{yannick.prie@univ-nantes.fr}
% 5th. author
\alignauthor
Ruben Verborgh\\
       \affaddr{Ghent University}\\
       \affaddr{iMinds -- Multimedia Lab}\\
       \affaddr{G.~Crommenlaan~8 bus~201}\\
       \affaddr{9050~Ghent, Belgium}\\
       \email{ruben.verborgh@ugent.be}
% 6th. author
\alignauthor
Rik Van de Walle\\
       \affaddr{Ghent University}\\
       \affaddr{iMinds -- Multimedia Lab}\\
       \affaddr{G.~Crommenlaan~8 bus~201}\\       
       \affaddr{9050~Ghent, Belgium}\\
       \email{rik.vandewalle@ugent.be}
}

\maketitle
\begin{abstract}
We present and evaluate
a~video annotation model and related technology stack
wholly based on HTML5 Web standards
and optimally leveraging native functionality
already present in modern Web browsers,
which allows for fully Read/Write-enabled
semantic and representational video annotations.
At time of video consumption, such video annotations
can already be existent as is the case
with annotations created by the video producer
(\emph{i.e.},~annotations of type \emph{Read}),
equally well as be created on-the-fly by the video consumer
(\emph{i.e.},~annotations of type \emph{Write}).
Our video annotation model allows for both,
semantic and representational annotations.
Semantic annotations are related to meaning in videos,
whereas representational annotations are related to
ways in which videos get represented to the viewer.
By leveraging JSON-LD and WebVTT as enabling technologies,
we are able to publish our annotations as Linked Data,
both for classical Semantic Web clients
as well as for common search engines.
We evaluate our technology stack and annotation model
based on a~state-of-the-art hypervideo model.
\end{abstract}

\category{ToDo}{\todo{category}}{}

\terms{\todo{terms}}

\keywords{\todo{keywords}}

\section{Introduction}

With the ever-growing number of implemented HTML5~\cite{berjon2013html5}
technologies in common Web browsers,
finally video has become a~first class citizen on the Web.
The \texttt{<video>} element has broad support
on all device classes,
starting from cellphones to tablets to full-size desktop PCs.
Videos can be intermixed with other multimedia content,
which allows for rich, interactive experiences%
\footnote{\url{http://www.chromeexperiments.com/}}
based on spatiotemporal annotations
not believed possible on the Web some years ago.
Yet the creation of such annotations
has mainly remained a~thing of Web content producers,
not content consumers or standard Web users.
While on common video platforms people
can leave comments \emph{about videos},
more rich annotations \emph{of videos} are not possible,
nor exists a~commonly accepted, open format to share annotations.

In this paper, we propose an annotation model
and related technology stack that allows both,
video consumers and video producers to create 
rich, interactive video experiences
in a~collaborative Read/Write manner.
We support representational and semantic annotations,
so that annotators can have an impact
on the look and feel of videos,
but also make semantic statements
about the meaning of the video itself, fragments of it,
or objects within it.
The created annotations are interchangeable
and not bound to a~specific video hosting platform,
as they are fully based on the Web standards
JSON-LD~\cite{sporny2013jsonld}
and WebVTT~\cite{pfeiffer2013webvtt}.

The remainder of this paper is organized as follows.
\todo{Describe paper structure}

\section{Related Work}

The online video hosting platform YouTube%
\footnote{\url{http://www.youtube.com/}}
lets video publishers add video annotations
in a~closed format%
~\cite{youtube2008annotations,youtube2010annotations}
that is proprietary to the platform.
In 2009, YouTube introduced a~feature called 
Collaborative Annotations%
~\cite{fink2009collaborativeannotations}
that allowed video consumers to collaboratively
create video annotations.
The feature was discontinued in 2010
\textit{``to make room for even more new features''}%
~\cite{youtube2010annotations}.
At time of writing, YouTube supports the following
annotation types: \emph{speech bubbles} for creating
pop-up speech bubbles with text,
\emph{spotlights} for highlighting areas in a~video
that react on mouse-over,
\emph{notes} for creating pop-up boxes containing text,
\emph{titles} for creating text overlays to title a~video,
and \emph{labels} for creating a~label to call out
and name a~specific part of a~video.
Notes, speech bubbles, and spotlights can contain \emph{links}
to other videos, the same video, channel pages, playlists,
or search results.
Similarly, they can also be linked to YouTube-specific actions
such as subscribe, compose message, and upload video response.
 
In~\cite{vandeursen2012mediafragmentannotations},
Van Deursen \emph{et~al.}\ present a~system
that combines Media Fragments URI~\cite{troncy2012mediafragments} and the Ontology for Media Resources~\cite{lee2012mediaontology}
within a~media-enabled HTML5~\cite{berjon2013html5}
Web application to convert rich media fragment annotations
into a~WebVTT~\cite{pfeiffer2013webvtt} file
that can be used by HTML5-enabled players
to show the annotations in a~synchronized way.
Concretely, their systems allows for linking, displaying
browsing, and bookmarking of media fragments,
providing structured annotations for media fragments,
and experiencing synchronized media and annotations.

\section{Enabling Technologies}

\subsection{Web Video Text Tracks format (WebVTT)}

The Web Video Text Tracks format (WebVTT,~\cite{pfeiffer2013webvtt})
is intended for marking up external text track resources mainly
for the purpose of captioning video content.
The recommended file extension is \texttt{vtt},
the MIME type is \texttt{text/vtt}.
WebVTT files are encoded in UTF-8 and
start with the required string \texttt{WEBVTT}.
Each file consists of items called \emph{cues}
that are separated by an empty line.
Each cue has a~start time and an end time in
\texttt{hh:mm:ss.milliseconds} format,
separated by a~stylized ASCII arrow \texttt{-}\texttt{->}.
The cue payload follows in the line after the cue timings part
and can span multiple lines.
Typically, the cue payload contains text,
but can also contain textual data serialization formats like JSON,
which later on in the paper we will show to be essential
for the proposed approach of semantic video annotation.
Spans of text associated with a~specific voice can be annotated
in form of WebVTT cue voice spans.
A WebVTT voice tag, denoted by \texttt{<v VOICE>}, has a~value,
which is the name of the voice.
Cues can optionally have unique IDs
followed by a~colon in the line
above the cue timings part.
\autoref{listing:webvtt} shows an exemplary WebVTT file.

\begin{lstlisting}[caption={Sample WebVTT file with two cues
    with the IDs \texttt{cue-vincent} and
    \texttt{cue-jules} respectively},
  label=listing:webvtt, float=h!]
WEBVTT

cue-vincent:
00:07:55.185 --> 00:08:00.247
<v Vincent>And you know what they call
a Quarter-Pounder with Cheese in Paris?

cue-jules:
00:08:00.290 --> 00:08:01.917
<v Jules>They don't call it
a Quarter-Pounder with Cheese?
\end{lstlisting}

\subsubsection{WebVTT in Web Browsers}

Compliant Web browsers~\cite{dutton2012trackelement}
support five different kinds of
WebVTT text tracks, specified by HTML5~\cite{berjon2013html5}
and listed in \autoref{table:texttrackkinds}.
In this paper, we especially focus on
text tracks of kind \texttt{metadata}
that are exclusively meant to be used from JavaScript and
that are not displayed by the user agent.

Both the HTML5 \texttt{<audio>} and \texttt{<video>} elements
have a~\texttt{textTracks} property
that returns a~\texttt{TextTrackList} of
\texttt{TextTrack} members, each of which correspond
to individual <track> elements.
A~\texttt{TextTrack} has a~\texttt{cues} property
that returns a~\texttt{TextTrackCueList} of individual
\texttt{TextTrackCue} items.
Cue data is accessible via properties like
\texttt{startTime}, \texttt{endTime} and,
most importantly, \texttt{text} to obtain a~cue's payload.
When the payload is in JSON format,
it can be parsed and made programmatically usable via the
\texttt{JSON.parse(cue.text)} function.

When a~media resource like a~video or audio is playing,
JavaScript events of the \texttt{TextTrack} and \texttt{TextTrackCue}
elements fire automatically when the \texttt{currentPosition}
of the media resource matches the
\texttt{startTime} or \texttt{endTime} of a~cue.
\texttt{TextTrack} elements fire \texttt{oncuechange} events,
whereas \texttt{TextTrackCue} elements fire
\texttt{onenter} and \texttt{onexit} events.
JavaScript applications can subscribe to these events
by adding event listeners to the elements in question.
Both \texttt{TextTrack} and \texttt{TextTrackCue} elements
can also be dynamically generated by JavaScript applications.

%PA: Might be a little too technical for a scientific paper,
%    but we'll see in the end what is absolutely necessary,
%    and what could be pruned.

\begin{table*}[h!]\footnotesize
\begin{tabular}{ r p{15cm} }
\textbf{WebVTT Kind} & \textbf{Description and Specified Default Behavior}\\

\texttt{subtitles} & Transcription or translation of the dialogue,
suitable for when the sound is available but not understood
(\emph{e.g.}, when the user does not understand the language). Overlaid on the video.\\

\texttt{captions} & Transcription or translation of the dialogue,
sound effects, relevant musical cues,
and other relevant audio information,
suitable for when sound is unavailable or not clearly audible
(\emph{e.g.}, when the user is deaf). Overlaid on the video;
labeled as appropriate for the hard-of-hearing.\\

\texttt{descriptions} & Textual descriptions of the video component
of the media resource, intended for audio synthesis
when the visual component is obscured, unavailable, or unusable
(\emph{e.g.}, when the user is blind).
Synthesized as audio.\\

\texttt{chapters} & Chapter titles, intended to be used for navigating
the media resource. Displayed as an interactive (potentially nested)
list in the user agent's interface.\\

\texttt{metadata} & Tracks intended for programmatic use from script.
Not displayed by the user agent.\\
\end{tabular}
  \caption{Different WebVTT text track kinds as specified by
    HTML5~\cite{berjon2013html5}}
  \label{table:texttrackkinds}
\end{table*}

\subsection{JSON-LD}

The \emph{JavaScript Object Notation}
(JSON\footnote{\url{http://json.org/}})
is a~(by now) language-independent textual syntax
for serializing objects, arrays, numbers, strings, booleans, and null.
\emph{Linked Data}~\cite{bizer2009linkeddata}
describes a~method of publishing structured data
so that it can be interlinked and become more useful,
which builds upon standard Web technologies such as HTTP, RDF and URIs.
Based on top of JSON, the
\emph{JavaScript Object Notation for Linked Data}
(JSON-LD,~\cite{sporny2013jsonld}) is a~method for transporting
Linked Data with a~smooth upgrade path from JSON to JSON-LD.
It is primarily intended to be a~way to use
Linked Data in Web-based programming environments,
to build interoperable Web services,
and to store Linked Data in JSON-based storage engines.
We use JSON-LD as payload of \texttt{TextTrackCue} elements.

\subsection{Media Fragments URI}

Media Fragments URI~\cite{troncy2012mediafragments}
specifies a~syntax for constructing media fragments URIs
and explains how to handle them when used over the HTTP protocol.
The syntax is based on the specification of
particular name-value pairs that can be used in URI query strings
and URI fragment identifiers to restrict a~media resource
to a~certain fragment.
Media Fragments as described in the basic version
of the specification supports temporal and spatial media fragments.
The \emph{temporal dimension} is denoted
by the parameter name \texttt{t} and specified as an interval
with a~begin time and an end time.
Either one or both parameters may be omitted,
with the begin time defaulting to 0 seconds
and the end time defaulting to the duration of the source media item.
The interval is half-open: the begin time is considered
part of the interval, whereas the end time is considered
to be the first time point that is not part of the interval.
The \emph{spatial dimension} selects
an area of pixels from media items.
In the current version of the specification,
only rectangular selections are supported.
Rectangles can be specified as pixel coordinates or percentages.
Rectangle selection is denoted by the parameter name \texttt{xywh}.
The value is either \texttt{pixel:} or \texttt{percent:}
(defaulting to \texttt{pixel:}) and four comma-separated integers.
The integers denote $x$, $y$, $width$, and $height$ respectively,
with $x = 0$ and $y = 0$ being the top left corner of the media item.
If \texttt{percent:} is used, $x$ and $width$ are interpreted
as a~percentage of the width of the original media item,
$y$ and $height$ are interpreted as a~percentage
of the original height.

\subsection{Ontology for Media Resources}

The Ontology for Media Resources~\cite{lee2012mediaontology}
serves to bridge different description methods of media resources
and to provide a~core set of descriptive properties.
Combined with Media Fragments URI,
this allows for making statements about media items
and fragments thereof.

\section{Implementation}

\subsection{Supported Annotation Types}

Currently, our system allows for semantic and representational
annotations of the following types.

\paragraph{Representational Annotations}

\begin{description}[leftmargin=*]
  \item[Spatial Fragments:] Via the \texttt{spatialFragment}
    annotation type, rectangular spatial fragments
    of the temporal fragment of the \texttt{video} element
    of the current \texttt{TextTrack} element can be selected.
    The selected spatial fragment is highlighted by a~rectangular
    frame on top of the video.
  \item[Style:] Annotations of
    type \texttt{style} allow for adding Cascading Style Sheet%
    \footnote{\url{http://www.w3.org/Style/CSS/}}
    styles to the corresponding temporal fragment of the
    \texttt{video} element of the current
    \texttt{TextTrack} element.
    This allows for a~range of visual effects,
    including (but not limited to) color filters,
    zoom effects, transparency, and 2D or 3D transformations
    and animations. 
  \item[Playback Rate:] The \texttt{playbackRate}
    annotation type allows for specifying the effective
    playback rate of the corresponding temporal fragment of the
    \texttt{video} element of the current
    \texttt{TextTrack} element.
    The playback rate is expressed as multiple or fraction
    of the intrinsic video speed.
  \item[Volume:] The \texttt{volume} annotation type allows for
    specifying the volume of the corresponding
    temporal fragment of the
    \texttt{video} element of the current
    \texttt{TextTrack} element on a~scale
    ranging from 0.0 (silent) to 1.0 (loudest).
  \item[HTML:] Via the \texttt{html} annotation type,
    freeform HTML code can be added on top of the
    corresponding temporal fragment of the
    \texttt{video} element of the
    current \texttt{TextTrack} element.
    This allows for the addition of graphical,
    textual, or combined overlays that can contain
    links to (temporal fragments of)
    other videos or within the current video.
\end{description}

\paragraph{Semantic Annotations}

\begin{description}[leftmargin=*]
  \item[Tags:] Semantic annotations of type \texttt{tags}
    allow for adding plain-text tags or semantic tags 
    in form of references to unique, well-defined concepts
    with their own URLs to the corresponding
    temporal fragment of the
    \texttt{video} element of the current
    \texttt{TextTrack} element.
  \item[Actors:] The \texttt{actors} annotation type allows for
    the actors in the corresponding temporal
    fragment of the \texttt{video} element of the current
    \texttt{TextTrack} element to be denoted.
    Plain-text or semantic labels 
    in form of references to unique, well-defined concepts
    with their own URLs are possible.
  \item[RDF Graph:] The \texttt{rdfGraph} annotation type
    allows for adding an arbitrary set of RDF triples
    to the corresponding temporal fragment of the
    \texttt{video} element of the current
    \texttt{TextTrack} element. In combination with the
    representational annotation type \texttt{spatialFragment},
    semantic annotations of objects
    in video fragments are possible.
\end{description}
  
\section{Evaluation and Discussion}

We evaluate or annotation model and related technology stack
based on a~state-of-the-art hypervideo model by
Sadallah \emph{et~al.}\ detailed in~\cite{sadallah2012hypervideo}
that resulted out of careful studies of prior art in the field.

\subsection{Hypervideo Model}

In~\cite{sadallah2012hypervideo}, Sadallah \emph{et~al.}\ define
\emph{hypervideo} as being \textit{``an interactive video-centric
hypermedia document built upon audiovisual content''}.
The authors identify three common hypervideo characteristics,
namely \emph{(i)}~\emph{interactivity}, which, \emph{e.g.},
can enable richer navigational possibilities, 
\emph{(ii)}~\emph{non-linearity}, which allows for advanced features
like video montages, and finally \emph{(iii)}~\emph{enrichments}
that include all sorts of supplementary material besides
and on top of hypervideos.
Sadallah \emph{et~al.}\ have examined hypervideo systems
of the recent years and found a~number of
recurring visualization and interaction patterns
that we will summarize in the following.

\begin{description}[leftmargin=*]
  \item[Video player and controls:] Hypervideo systems by definition
    provide one or multiple video players, however,
    the corresponding video controls are not necessarily exposed.
  \item[Timeline:] A~timeline is the spatial representation
    of temporally situated metadata in a~video.
    The most common timeline pattern shows the
    time along the x-axis and corresponding metadata along the y-axis.
  \item[Textual or graphical overlay:] Additional textual or graphical
    information can be displayed in form of overlays on the video.
    Overlays can also serve as external or video-internal hyperlinks,
    referred to as hotspots.
  \item[Textual or graphical table of contents:]
    If a~video is logically separated
    into different parts, a~table of contents lists these
    in textual or graphical form and makes them navigable
    or visually summarizes them, referred to as video map.
  \item[Transcript:] The textual document
    of the transcribed audiovisual content of a~video 
    allows for following along the video by reading
    and also serves for navigating to the corresponding
    parts of the video.
\end{description}  

\section{Future Work and Conclusions}

\section*{Acknowledgments}

The research presented in this paper
was partially supported by the ANR project
\emph{Spectacle En Ligne(s)}, project reference
\mbox{ANR-12-CORP-0015}.

\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
