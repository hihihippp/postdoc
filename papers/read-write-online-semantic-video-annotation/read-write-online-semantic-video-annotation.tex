\documentclass{sig-alternate}

\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=2em,belowskip=1em,abovecaptionskip=0.5em,belowcaptionskip=0.5em,
        framexbottommargin=-1em,basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

\lstdefinelanguage{JavaScript}{
  keywords={push, typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{darkgray},
  stringstyle=\color{red},
  morestring=[b]',
  morestring=[b]"
}

% linewrap symbol
\usepackage{color}
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}

\def\JSONLD{\mbox{JSON-LD}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{International World Wide Web Conference}{2014 Seoul, Korea}
\CopyrightYear{2014} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Read/Write Semantic and Presentation-Oriented Online Video Annotation by Leveraging WebVTT and \JSONLD}

\numberofauthors{6}

\author{
% 1st. author
\alignauthor
Thomas Steiner\titlenote{Thomas Steiner's second affiliation is \emph{T.~Steiner, Google Germany GmbH, ABC-Str.~19, 20354 Hamburg, Germany}}\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{tsteiner@liris.cnrs.fr}
% 2nd. author
\alignauthor
Pierre-Antoine Champin\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{champin@liris.cnrs.fr}
% 3rd. author
\alignauthor
Benoît Encelle\\
       \affaddr{Université de Lyon} 
       \affaddr{CNRS Université Lyon~1}\\
       \affaddr{LIRIS, UMR5205}\\
       \affaddr{F-69622 France}\\
       \email{encelle@liris.cnrs.fr}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor
Yannick Prié\\
       \affaddr{Université de Nantes}\\
       \affaddr{LINA - UMR 6241}\\
       \affaddr{CNRS, France}\\
       \email{yannick.prie@univ-nantes.fr}
% 5th. author
\alignauthor
Ruben Verborgh\\
       \affaddr{Ghent University}\\
       \affaddr{iMinds -- Multimedia Lab}\\
       \affaddr{G.~Crommenlaan~8 bus~201}\\
       \affaddr{9050~Ghent, Belgium}\\
       \email{ruben.verborgh@ugent.be}
% 6th. author
\alignauthor
Rik Van de Walle\\
       \affaddr{Ghent University}\\
       \affaddr{iMinds -- Multimedia Lab}\\
       \affaddr{G.~Crommenlaan~8 bus~201}\\       
       \affaddr{9050~Ghent, Belgium}\\
       \email{rik.vandewalle@ugent.be}
}

\maketitle
\begin{abstract}
We present and evaluate a~video annotation model
and related technology stack that is entirely based
on HTML5 Web standards and leverages native functionality
present in Web browsers.
This allows for fully Read/Write-enabled video annotations.
At time of video consumption, such video annotations
can already exist (\emph{i.e.}, \emph{Read} annotations),
as well as be created on-the-fly by the video consumer
(\emph{i.e.}, \emph{Write} annotations).
Our video annotation model allows for both,
semantic and presentation-oriented annotations.
Semantic annotations are related to meaning in videos,
whereas pre\-sen\-tation-oriented annotations are related to
ways in which videos get displayed to the viewer.
By leveraging the technologies \JSONLD~and WebVTT,
we are able to publish our annotations as Linked Data
both for classic Semantic Web clients
as well as common search engines.
We evaluate our annotation model
with a~state-of-the-art hypervideo model.
\end{abstract}

\category{H.5.1}{Multimedia Inform. Syst.}{Eval./methodology}

\terms{Experimentation, Design, Standardization}

\keywords{Video annotation, WebVTT, \JSONLD, media fragments}

\section{Introduction}

With the ever-growing number of implemented HTML5~\cite{berjon2013html5}
technologies in common Web browsers,
finally video has become a~first class citizen on the Web.
The \texttt{<video>} element has broad support
on all device classes,
starting from cellphones to tablets to full-size desktop PCs.
Videos can be intermixed with other multimedia content,
which allows for rich, interactive experiences%
\footnote{Showcase of interactive Web video experiments:
\url{http://www.chromeexperiments.com/tag/video/}}
based on spatiotemporal annotations
not believed possible on the Web some years ago.
Yet the creation of such annotations
has mainly remained a~thing of Web content producers,
leaving out content consumers or common Web users.
While on established video platforms people
can leave comments \emph{about videos},
more rich annotations \emph{of videos} are not possible,
nor exists a~commonly accepted open format
to share annotations on the Web.

In this paper, we propose an annotation model
and related technology stack that allows both
video consumers and producers to create 
rich, interactive video experiences
in a~collaborative Read/Write manner.
We support presentation-oriented and semantic annotations,
so that annotators can influence
the presentation style of videos on the one hand,
but on the other also make semantic statements
about the meaning of the video itself, fragments of it,
or objects within it.
Such annotations are interchangeable
and not bound to specific video hosting platforms,
as they are based on the Web standards
\JSONLD~\cite{sporny2013jsonld}
and WebVTT~\cite{pfeiffer2013webvtt}.

\section{Technologies Overview}

For our approach to online video annotation,
we make use of a~young---yet mature
when looked at from an implementation viewpoint---%
stack of enabling technologies that we will introduce
and provide technical details on in the following.

\paragraph{Web Video Text Tracks format (WebVTT)}

The Web Video Text Tracks format (WebVTT,~\cite{pfeiffer2013webvtt})
is intended for marking up external text track resources mainly
for the purpose of captioning video content.
The recommended file extension is \texttt{vtt},
the MIME type is \texttt{text/vtt}.
WebVTT files are encoded in UTF-8 and
start with the required string \texttt{WEBVTT}.
Each file consists of items called \emph{cues}
that are separated by an empty line.
Each cue has a~start time and an end time in
\texttt{hh:mm:ss.milliseconds} format,
separated by a~stylized ASCII arrow \texttt{-}\texttt{->}.
The cue payload follows in the line after the cue timings part
and can span multiple lines.
Typically, the cue payload contains plain text,
but can also contain textual data serialization formats like JSON,
which later on in the paper we will show is essential
for our proposed approach to semantic video annotation.
Spans of text associated with a~specific voice can be annotated
in form of WebVTT cue voice spans.
A~WebVTT voice tag, denoted by \texttt{<v VOICE>}, has a~value,
which is the name of the voice.
Cues can optionally have unique IDs
followed by a~colon in the line
above the cue timings part.
\autoref{listing:webvtt} shows an exemplary WebVTT file.

\begin{lstlisting}[caption={Sample WebVTT file with two cues},
  label=listing:webvtt, float=b!]
WEBVTT

cue_vincent_1:
00:07:55.185 --> 00:08:00.247
<v Vincent>And you know what they call
a Quarter-Pounder with Cheese in Paris?

cue_jules_2:
00:08:00.290 --> 00:08:01.917
<v Jules>They don't call it
a Quarter-Pounder with Cheese?
\end{lstlisting}

\begin{table}[b!]\footnotesize
\begin{tabular}{ r p{5.5cm} } % ACM column width 8.45cm, 0.83cm gutter
\textbf{WebVTT Kind} & \textbf{Description and Default Behavior}\\

\texttt{subtitles} & Transcription or translation of speech,
suitable for when sound is available but not understood. Overlaid on the video.\\

\texttt{captions} & Transcription or translation of the dialogue,
sound effects, and other relevant audio information,
suitable for when sound is unavailable or not clearly audible. Overlaid on the video;
labeled as appropriate for the hard-of-hearing.\\

\texttt{descriptions} & Textual descriptions of the video component
of the media resource, intended for audio synthesis
when the visual component is obscured, unavailable, or unusable.
Synthesized as audio.\\

\texttt{chapters} & Chapter titles, intended to be used for navigating
the media resource. Displayed as an interactive (potentially nested)
list in the user agent's interface.\\

\texttt{metadata} & Metadata intended for use from script context.
Not displayed by user agent.\\
\end{tabular}
  \caption{WebVTT text track kinds in HTML5~\cite{berjon2013html5}}
  \label{table:texttrackkinds}
\end{table}

WebVTT-compliant Web browsers~\cite{dutton2012trackelement}
support five different kinds of
WebVTT text tracks, specified by HTML5~\cite{berjon2013html5}
and listed in \autoref{table:texttrackkinds}.
In this paper, we especially focus on
text tracks of kind \texttt{metadata},
meant to be used from a~script context and
that are not displayed by the user agent.

Both the HTML5 \texttt{<audio>} and \texttt{<video>} elements
have a~\texttt{textTracks} property
that returns a~\texttt{TextTrackList} of
\texttt{TextTrack} members, each of which correspond
to individual <track> elements.
A~\texttt{TextTrack} has a~\texttt{cues} property
that returns a~\texttt{TextTrackCueList} of individual
\texttt{TextTrackCue} items.
Cue data is accessible via properties like
\texttt{startTime}, \texttt{endTime} and,
most importantly, \texttt{text} to obtain a~cue's payload.
When the payload is in JSON format,
it can be parsed via the
\texttt{JSON.parse()} function.

When a~media resource like a~video or audio is playing,
JavaScript events of the \texttt{TextTrack} and \texttt{TextTrackCue}
elements fire automatically when the \texttt{currentPosition}
of the media resource matches the
\texttt{startTime} or \texttt{endTime} of a~cue.
\texttt{TextTrack} elements fire \texttt{oncuechange} events,
whereas \texttt{TextTrackCue} elements fire
\texttt{onenter} and \texttt{onexit} events.
JavaScript applications can subscribe to these events.
Both \texttt{TextTrack} and \texttt{TextTrackCue} elements
can also be dynamically generated by JavaScript applications.

\paragraph{\JSONLD}

The \emph{JavaScript Object Notation}%
\footnote{JavaScript Object Notation: \url{http://json.org/}}
(JSON)
is a~(despite the name) language-independent textual syntax
for serializing objects, arrays, numbers, strings, booleans, and null.
\emph{Linked Data}~\cite{bizer2009linkeddata}
describes a~method of publishing structured data
so that it can be interlinked and become more useful,
which builds upon standard Web technologies such as HTTP, RDF and URIs.
Based on top of JSON, the
\emph{JavaScript Object Notation for Linked Data}
(\JSONLD,~\cite{sporny2013jsonld}) is a~method for transporting
Linked Data with a~smooth upgrade path from JSON to \JSONLD.
It is primarily intended to be a~way to use
Linked Data in Web-based programming environments,
to build interoperable Web services,
and to store Linked Data in JSON-based storage engines.
We use \JSONLD\ as semantically rich content in the payload of \texttt{TextTrackCue}s.

\paragraph{Media Fragments URI}

Media Fragments URI~\cite{troncy2012mediafragments}
specifies a~syntax for constructing media fragments URIs
and explains how to handle them over the HTTP protocol.
The syntax is based on the specification of
name-value pairs that can be used in URI query strings
and URI fragment identifiers to restrict a~media resource
to a~certain fragment.
Media Fragments URI supports temporal and spatial media fragments.
The \emph{temporal dimension} is denoted
by the parameter name \texttt{t} and specified
as a~half-open interval with begin time and end time
that may also be omitted,
with the begin time defaulting to 0 seconds
and the end time defaulting to the media item's duration.
The \emph{spatial dimension} selects
a~rectangular area of pixels from media items.
Rectangles can be specified as pixel coordinates or percentages.
Rectangle selection is denoted by the parameter name \texttt{xywh}.
The value is either \texttt{pixel:} or \texttt{percent:}
(defaulting to \texttt{pixel:})
followed by four comma-separated integers.
The integers denote $x$, $y$, $width$, and $height$ respectively,
with $x = 0$ and $y = 0$ being the top left corner of the media item.
If \texttt{percent:} is used, $x$ and $width$ are interpreted
as a~percentage of the width of the original media item,
$y$ and $height$
of the original height.

\paragraph{Ontology for Media Resources}

The Ontology for Media Resources~\cite{lee2012mediaontology}
serves to bridge different description methods of media resources
and to provide a~core set of descriptive properties.
It also defines mappings to common metadata formats.
Combined with Media Fragments URI,
this allows for making ontologically anchored statements
about media items and fragments thereof.

\section{Annotation Model}

\paragraph{General Concepts of the Annotation Model}

Our annotation model uses subject-predicate-object expressions.
The simplest annotation is thus a~single triple.
The possible values for the \emph{subject} can be \emph{(i)}~the video
URI itself or---in case of multiple URLs for a~video available
in different encodings with multiple \texttt{<source>} tags---%
the URI with fragment identifier of the \texttt{<video>} tag
that then requires a~unique ID,
\emph{(ii)}~the Media Fragment URI of a~temporal and/or spatial
media fragment of the original media item.
We have no constraints in place for the \emph{predicate}
and {object}, we thus make a~sort of open world assumption by
supporting certain pre-defined values for predicate and object
listed below in our interpretation layer in the video view,
and simply ignoring unknown ones,
which permits others to extend or even completely replace
our interpretation layer.

\paragraph{Supported Presentation-Oriented Annotation Types}

\begin{description}[leftmargin=*]
  \item[Style:] Annotations of
    type \texttt{style} allow for applying styles in the syntax of
    Cascading Style Sheets%
    \footnote{Cascading Style Sheets:
    \url{http://www.w3.org/Style/CSS/}} (CSS)
    to a~media fragment.
    This allows for a~range of visual effects,
    including (but not limited to) color filters,
    zoom effects, transparency, and 2D or 3D transformations
    and animations. 
  \item[Playback Rate:] The \texttt{playbackRate}
    annotation type allows for specifying the effective
    playback rate of a~media fragment.
    The playback rate is expressed as a~floating point
    multiple or fraction of the intrinsic video speed.
  \item[Volume:] The \texttt{volume} annotation type allows for
    specifying the volume of a~media fragment on a~scale
    from 0.0 to 1.0.
  \item[HTML:] Via the \texttt{html} annotation type,
    freeform HTML code can be added on top of a~media fragment.
    This allows for the addition of graphical,
    textual, or combined overlays that can contain
    links to (temporal fragments of)
    other videos or within the current video.
\end{description}

\paragraph{Supported Semantic Annotation Types}

\begin{description}[leftmargin=*]
  \item[Plain Text Tags:] Annotations of type
    \texttt{tags} allow for add\-ing plain text tags
    to a~media fragment.
    Internally, they are represented as Common Tag%
    ~\cite{commontag2009spec} format \texttt{ctag:label}. 
  \item[Semantic Tags:] Annotations of type
    \texttt{semanticTags} allow for adding semantic tags
    to a~media fragment. 
    Unlike plain text tags, semantic tags are references to
    well-defined concepts complete with their own URLs.
    Internally they are represented as Common Tag%
    ~\cite{commontag2009spec} format \texttt{ctag:means}.     
  \item[Contributors:] The \texttt{contributors} annotation type
    al-\linebreak lows for denoting the contributors in a~media fragment.
    Internally they are represented as
    Ontology for Media Resources~\cite{lee2012mediaontology}
    format \texttt{ma:contributors}.    
  \item[Description:] The \texttt{description} annotation type
    allows for describing a~media fragment with plain text.
    Internally it is represented as
    Ontology for Media Resources~\cite{lee2012mediaontology}
    format \texttt{ma:description}.    
  \item[Named Graph:] The \texttt{namedGraph} annotation type
    allows for adding a~set of RDF triples
    to a~media fragment.
    Internally it is represented as JSON-LD%
    ~\cite{sporny2013jsonld} named graph.
\end{description}
  
\section{Evaluation and Discussion}

We evaluate or annotation model and related technology stack
based on a~state-of-the-art hypervideo model by
Sadallah \emph{et~al.}\ detailed in~\cite{sadallah2012hypervideo}
that resulted out of careful studies of prior art in the field.

\subsection{Hypervideo Model}

In~\cite{sadallah2012hypervideo}, Sadallah \emph{et~al.}\ define
\emph{hypervideo} as being \textit{``an interactive video-centric
hypermedia document built upon audiovisual content''}.
The authors identify three common hypervideo characteristics,
namely \emph{(i)}~\emph{interactivity}, which, \emph{e.g.},
can enable richer navigational possibilities, 
\emph{(ii)}~\emph{non-linearity}, which allows for advanced features
like video montages, and finally \emph{(iii)}~\emph{enrichments}
that include all sorts of supplementary material besides
and on top of hypervideos.
Sadallah \emph{et~al.}\ have examined hypervideo systems
of the recent years and found a~number of
recurring visualization and interaction patterns
that we will summarize in the following.

\begin{description}[leftmargin=*]
  \item[Video player and controls:] Hypervideo systems by definition
    provide one or multiple video players, however,
    the corresponding video controls are not necessarily exposed.
  \item[Timeline:] A~timeline is the spatial representation
    of temporally situated metadata in a~video.
    The most common timeline pattern shows the
    time along the x-axis and corresponding metadata along the y-axis.
  \item[Textual or graphical overlay:] Additional textual or gra\-phical
    information can be displayed in form of overlays on the video.
    Overlays can also serve as external or video-internal hyperlinks,
    referred to as hotspots.
  \item[Textual or graphical table of contents:]
    If a~video is logically separated
    into different parts, a~table of contents lists these
    in textual or graphical form and makes them navigable
    or visually summarizes them, referred to as video map.
  \item[Transcript:] The textual document
    of the transcribed audiovisual content of a~video 
    allows for following along the video by reading
    and also serves for navigating to the corresponding
    parts of the video.
\end{description}  

\section{Related Work}

The online video hosting platform YouTube%
\footnote{YouTube: \url{http://www.youtube.com/}}
lets video publishers add video annotations
in a~closed proprietary format%
~\cite{youtube2008annotations,youtube2010annotations}.
At time of writing, YouTube supports the following
annotation types: \emph{speech bubbles} for creating
pop-up speech bubbles with text,
\emph{spotlights} for highlighting areas in a~video
that react on mouse-over,
\emph{notes} for creating pop-up boxes containing text,
\emph{titles} for creating text overlays to title a~video,
and \emph{labels} for creating a~label to call out
and name a~specific part of a~video.
Notes, speech bubbles, and spotlights can contain \emph{links}
to other videos, the same video, channel pages, playlists,
or search results, as well as to actions
such as subscribe, compose message, and upload video response.
From 2009 to 2010, YouTube had a~feature called 
Collaborative Annotations%
~\cite{fink2009collaborativeannotations}
that allowed video consumers to collaboratively
create video annotations.
It was discontinued \textit{``to make room for even more new features''}%
~\cite{youtube2010annotations}.
 
In~\cite{vandeursen2012mediafragmentannotations},
Van Deursen \emph{et~al.}\ present a~system
that combines Media Fragments URI~\cite{troncy2012mediafragments} and the Ontology for Media Resources~\cite{lee2012mediaontology}
in an HTML5~\cite{berjon2013html5}
Web application to convert rich media fragment annotations
into a~WebVTT~\cite{pfeiffer2013webvtt} file
that can be used by HTML5-enabled players
to show the annotations in a~synchronized way.
Their systems allows for linking, displaying,
browsing, and bookmarking of media fragments,
providing structured annotations for media fragments,
and experiencing synchronized media and annotations.

The Synchronized Multimedia Integration Language\linebreak
(SMIL,~\cite{bulterman2006smil})
defines an XML-based language that allows authors
to write interactive multimedia presentations.
Using SMIL, an author can describe the temporal behavior
of a~multimedia presentation, associate hyperlinks
with media objects, and describe the layout
of the presentation on a~screen.
% SMIL components can be used for integrating timing
% into XHTML~\cite{pemberton2002xhtml}
% and into SVG~\cite{ferraiolo2003svg}.
SMIL features are not commonly supported 
in modern Web browsers and special plugins or native software
are required for their consumption,
albeit in 2002 there was a~mostly Microsoft-led attempt
in form of the HTML+TIME submission~\cite{schmitz1998htmltime}
and, based thereon, the XHTML+SMIL Profile note%
~\cite{newman2002xhtmlsmil}
to integrate SMIL semantics with (X)HTML and CSS.
Despite its initially active role in the W3C SMIL working group,
Microsoft later decided to cease SMIL support
due to a~perceived overlap with several existing standards.

The Component-based Hypervideo Model
(CHM,~\cite{sadallah2012hypervideo})
proposes a~formal model for hypervideo documents
and offers a~practical framework that allows for
the design of Web-oriented hypervideos.
CHM annotations are defined as any kind of data associated to
a~logical spatiotemporal video fragment.
A~version of the proposed framework
has been released in form of the WebCHM prototype
that integrates with existing Web technologies
via a~declarative syntax.
Support for temporal semantics is ensured through  
an implementation of SMIL Timesheets~\cite{vuorimaa2012timesheets}
in a~JavaScript library.

Popcorn.js\footnote{Popcorn.js: \url{http://popcornjs.org/}}
is an HTML5 media framework written in JavaScript
aimed at filmmakers, Web developers,
and users interested in creating
time-based interactive media on the Web.
Popcorn.js is part of Mozilla's Popcorn project
and adds interactivity and context to online video
by letting users link social media, news feeds,
data visualizations, and other content directly to moving images.
The Mozilla PopcornMaker%
\footnote{PopcornMaker: \url{https://popcorn.webmaker.org/}}
is an interactive Web authoring environment
that allows for videos to be annotated on a~video timeline.

\section{Future Work and Conclusions}

Extend CSS by temporal component

Meta-annotations

Conflicting annotatiuons

\section*{Acknowledgments}

The research presented in this paper
was partially supported by the ANR project
\emph{Spectacle En Ligne(s)}, project reference
\mbox{ANR-12-CORP-0015}.

\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
